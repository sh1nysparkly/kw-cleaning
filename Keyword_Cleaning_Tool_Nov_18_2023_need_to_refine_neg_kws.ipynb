{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2xQe7XNSNyf"
      },
      "source": [
        "\n",
        "#AMA Kewyword Cleaner & Keyword Clustering Tool V3\n",
        "\n",
        "- tag by intent, location\n",
        "- simple spell check and flag for review\n",
        "- simple language check to flag non-English rows\n",
        "- OPT: removes duplicate keywords (URL insensitive)\n",
        "- removes negative KWs (based on chosen dictionary/relevant BU)\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "***NEXT: choose custom intent classifiers & clustering by dictionary/BU***\n",
        "\n",
        "***NEXT: support all upload types from Conductor***\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Start Instructions\n",
        "Run all the cells and upload a CSV export.\n",
        "Runtime > Run All (Control + F9)\n",
        "\n",
        "### Works with the Following Exports out the Box\n",
        "\n",
        "*   Ahrefs.com (Keyword Export / Site Explorer Export)\n",
        "*   SEMRush.com\n",
        "*   Search Console (Coverage Report CSV Export (Queries.csv))\n",
        "*   AdWords Search Terms Report .csv or Excel format (Beta)\n",
        "*   A simple single column .txt / csv file with keywords (Header or Headerless)\n",
        "\n",
        "### File Formats\n",
        "*   utf-8/utf-16/csv/xls/xlsx/xlsm/xlsb/odf/ods/odt"
      ],
      "metadata": {
        "id": "fK6H6kZBboQT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPAaQtNdMqlb",
        "outputId": "bf93efcd-17be-47ac-a299-87d8bd3d88f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.3.post1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.23.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting polyfuzz[fast]\n",
            "  Downloading polyfuzz-0.4.2-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (1.11.3)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.41.1 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (4.66.1)\n",
            "Requirement already satisfied: joblib>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (1.3.2)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (3.7.1)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (0.12.2)\n",
            "Collecting rapidfuzz>=0.13.1 (from polyfuzz[fast])\n",
            "  Downloading rapidfuzz-3.5.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2.post1 in /usr/local/lib/python3.10/dist-packages (from polyfuzz[fast]) (1.2.2)\n",
            "Collecting sparse-dot-topn>=0.2.9 (from polyfuzz[fast])\n",
            "  Downloading sparse_dot_topn-0.3.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (4.44.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.2.2->polyfuzz[fast]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.25.3->polyfuzz[fast]) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2.post1->polyfuzz[fast]) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->polyfuzz[fast]) (1.16.0)\n",
            "Installing collected packages: rapidfuzz, sparse-dot-topn, polyfuzz\n",
            "Successfully installed polyfuzz-0.4.2 rapidfuzz-3.5.2 sparse-dot-topn-0.3.6\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (5.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=05560344a992c0d39235b233e43a7f3293cd7d8c86e3bc5759bdd88435d48748\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas\n",
        "!pip install polyfuzz[fast]\n",
        "!pip install chardet\n",
        "!pip install tqdm\n",
        "!pip install langdetect\n",
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9GPFNiTcMZnS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "from google.colab import files\n",
        "from polyfuzz import PolyFuzz\n",
        "import chardet\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from langdetect import detect\n",
        "from langdetect import DetectorFactory\n",
        "from spellchecker import SpellChecker\n",
        "# Set the langdetect factory seed for reproducibility\n",
        "DetectorFactory.seed = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "metadata": {
        "id": "xAjknFtjMvwJ"
      },
      "outputs": [],
      "source": [
        "# rename the parent cluster name using the keyword with the highest search volume (recommended)\n",
        "parent_by_vol = True\n",
        "drop_site_links = False\n",
        "drop_image_links = False\n",
        "sim_match_percent = 1\n",
        "url_filter = \"\"\n",
        "min_volume = 0  # set the minimum search volume / impressions to filter on"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL3DIozl65mR"
      },
      "source": [
        "## **Define Negative Keywords to Clean**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "negkw_sets = {\n",
        "    \"insurance\": \"lawyer|sue|lawsuit\",\n",
        "    \"registries\": \"girl|signin|login|sign in|log in|portal|www|http|link|adopt|dog|cow|rodeo|vision|cerb|poppy|shoe|barber|chef|gold|sin \",\n",
        "    \"DE\": \"claim|lawyer\",\n",
        "    # Add more sets as needed\n",
        "}"
      ],
      "metadata": {
        "id": "Qk1Hb6qRWbeV"
      },
      "execution_count": 463,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nonabkw_filter = \"^bc | bc | bc$|^sk | sk | sk$| ns$|bathurst|afric|manitoba|quebec|^us | us$| us |halifax|markham| mb$|^mb |mexico|mexican|^ns |manitoba|quebec|nova scotia|^ns |markham|hst|pst|qst|^sk |^nl |^nb | ou | is$| in$| a$|^us | us$|california|2015|2016|2017|2018|2019|2020|2021|2022|nwt|^a |america|barrie|brampton|british|brunswick|fredericton|halifax|hamilton|hartford|kitchener| la$|manitoba|michigan|mile|missisauga|mkmk|moncton|montreal|newfoundland| nl|ontario|ottawa|pei|plano|prince|quebec|regina|sask|nova|spokane|state|sudbury|toronto|troy|tx|vancouver|waterloo|windsor|winnipeg| us$|^us |sturgeon|qc|pq\""
      ],
      "metadata": {
        "id": "pLuHmPrtgbSA"
      },
      "execution_count": 464,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "commonlymissedfr_filter = \"incorporer|livre|marque|marques|domaine|registraire|capitale|canadienne|societe|sans|registre|é|à\""
      ],
      "metadata": {
        "id": "3LW6aIWhj1o8"
      },
      "execution_count": 465,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9BaiYKmOxD_"
      },
      "source": [
        "## **Define Intent Classifiers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "metadata": {
        "id": "sMlwHf9dOwsn"
      },
      "outputs": [],
      "source": [
        "info_filter = \"what|where is|where does|why|when|who|how does|how to|which|tip|reddit|guide|tutorial|ideas|example|learn|wiki|in mm|in cm|in ft|in feet|question\"\n",
        "comm_invest_filter = \"best| vs|list|compare|review|list|^top|difference between|alternative|competitor|case study|rating|^rank|plan|company|companies|advis|agen\"\n",
        "trans_filter = \"affordable|purchase|bargain|cheap|deal|value|buy|shop|coupon|discount|price|pricing|order|sale|cost|how much|estimat|quote|rate|calculat|get|add \""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylfuaXmk6o_G"
      },
      "source": [
        "## **Define Custom Cluster Roots**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 491,
      "metadata": {
        "id": "IqDvR2TL6nwJ"
      },
      "outputs": [],
      "source": [
        "alberta_filter = \"^ab |alberta|near|local|edmonton|calgary|lethbridge|camrose|mcmurray|grande|prairie|medicine|deer|sherwood|albert$|willow|kingsway|sunridge|manning|crowfoot|shawnessy|airdrie|leduc|ponoka|wetask|hythe|spruce|okotok|high river\"\n",
        "nearme_filter = \"near me|local\"\n",
        "cities_filter = \"edmonton|calgary|lethbridge|camrose|mcmurray|grande|prairie|medicine|deer|sherwood|albert$|willow|kingsway|sunridge|manning|crowfoot|shawnessy\"\n",
        "neighbor_filter = \"airdrie|leduc|ponoka|wetask|hythe|spruce|okotok|high river\"\n",
        "branded_filter = \"ama|caa|aaa|motor association\"\n",
        "insurance_filter = \"accept|covered|claim| no |without\"\n",
        "registry_filter = \"cvip|marriage|abstract|handicap| ahs|^ahs |carfax|notar|lein|lien|^lean|vin|^id | id |birth|certificate|passport|identification|incorpor|learner|test|plate|inspect||class |gdl|corpor|dissolu|bill |black|shred|health care card|health card|registr|license|fine|\"\n",
        "de_filter = \"driv|road test|train|practice|quiz|instruct|lesson|school|fail|parallel|\"\n",
        "vehicle_filter = \"auto|car |truck|vehicle|motorcycle|bike|atv|boat|moped|snowmobile|motor insurance|motor car|car motor| car$| cars$|scooter|motors|plpd|fault\"\n",
        "property_filter = \"home|house|condo|modular|rv|recreational|motorhome|trailer|vacation|tennant|tenant|rent|apartment|property|content|antique\"\n",
        "pet_filter = \"pet|cat|dog|puppy|animal|feline\"\n",
        "health_filter = \"life|health|dental|accident|disability|illness|even|medical|wedding|extended|injur|benefit\"\n",
        "commercial_filter = \"business|commercial|work|liab|agri|farm|corporat\"\n",
        "partner_filter = \"depot|fountain|freshco|husky|^bills|grocery|gift card|keg|kernel|chatters|master|reload| inn|creative door| 60|marmot|pita|f2|fionn|enterprise|edible|mario|dulux|^don|derrick|derk|73|pizza|clover|eurocraft|staples|zoo|tutti|mobil|promo|lenscraft|pennington|101|mobility|a 1|academy hear|acadia|auto service|access automotive|accurate transmission|ac hotels|action muffler|best western|alamo|alberta driveline|marine|massage|aldo|alexanian|carpet|flooring|all lock rescue ltd|aloft|altitude sports|amj campbell|amsteam carpet cleaning inc|andaz|anytime fitness|a r automotive|ardene|ashley homestore|autograph collection|auto imports|auto trac alignment|babycakes cupcakery|bento sushi|best choice automotive|best western|best western hotels resorts gift card|best western plus|best western premier|bills garage|bliss yoga spa ltd|blondies gift garden ltd|^bou|bowl alberta|boyd royal vista|bravado designs|browns chrysler|bruce stewarts auto repair centre|bumble and bumble|bw premier collection|caddy auto service|calgary lock safe|calgary renovation show|calgary zoo|canada golf card|canadian rockies hot springs|canadream|canmore river adventures ltd|canon estore canada|canopy|cantrust auto repair|can west transmissions parts|resort|care cleaners|carstar|carters|oshkosh|castle toys|castrol|centre street|century motors|certapro|certified radio|cetus automotive|chatters|chopped leaf|cineplex|city collision|city wide|radiator|clarins|cleaningpros|clean x carpet upholstery cleaning ltd|clinique|cloverdale paint|coast appliances|columbia sportswear|computerized autopro|connect hearing|conrad hotels resorts|contemporary coachworks|cookies by george|cookies by george gift card|country automotive specialists|courtyard by marriott|cozey|creative door services ltd|crowfoot minit tune brake|csn collision centres|cub cadet|curio|dale adams automotive|dalhousie auto service|dandy auto marine rv ltd|davidstea|deco windshield repair|dell|delta|deluxe rv service sales|denham ford sales|derks formals|derrick dodge|dignity memorial|dons tire automotive repair|dot transmissions|doubletree hotels|dougs|dulux|earls|eastern mechanics|eastside dodge chrysler jeep ram fiat|east side marios|edible arrangements|edition|edmonton elks|edmonton renovation show|edmonton wedding party centre|element|elite|marketing|embassy suites|enterprise|enterprise truck rental|escape 60|ethos bridal group|eurocraft collision center calgary ltd|evergreen lawn services|executive residency by best western|expressions at home|extreme pita|f2 furnishings|fabricville|fairfield inns|fairfield inn suites|fairmont hotels resorts|fionn maccools|first choice collision|fishmans care cleaners|fix auto|flirt cupcakes|foothills mechanical services|forever 21|fort calgary|fort george and buckingham house provincial historic site|fountain tire calgary bowness|four points by sheraton|frank slide interpretive centre|freedom ford|freshii|fusion collision|garage 104|gate ave service|gaylord hotels|gershaw auto|glassmasters autoglass ltd|glo|globo shoes|godiva|go rv|grande prairie auto repair|grand hyatt|great canadian oil change|grower direct alberta|gm|hampton|hart|harveys|buffalo|herbers|hilton|dunvegan|hi tech|home2|home hardware|homewood|horton|h r|hudsons|hyatt|inabuggy|indigo|infinity|innisfail|integra|international|irobot|it cosmetics|jacks|j adams autobody|jamieson|vitamins|jasper raft|skytram|jds|jm|joe fresh|jugo juice|just junk|jw marriott|kal tire|kernels gift card|keurig|kingsgate automotive|kingsway toyota|k js custom granite inc|kleiber automotive|knibbe automotive repair|koch ford lincoln|kudos for wood furniture|lacombe auto service|lake louise ski resort and summer gondola|lakeview husky|lamb ford sales|lanco me|landmark cinemas|lego|le meridien|lenscrafters|linen chest|loblaw optical|lole|londonderry auto service|londonderry collision|lone wolf mechanical|lube city|lube city gift card|luxury collection|macleod auto truck repair|malik auto centre|mancuso carpet and upholstery cleaning|maranello auto refinishing ltd|marble slab creamery|marmot basin|marriott hotels and resorts|master cleaner|matt nat|mcmaster|mgm|lube|millwoods auto inspection repairs|ming shine|mint auto service|minute muffler|mi vida massage and wellness|mobil 1 lube express|monster mini golf|montanas|monza auto|morinville tirecraft|mount pleasant auto service|moxy hotels|mri autocare|mr lube|mudders wash|napa|national|national transmission|natura market|new asian village|new york fries|nitehawk year round adventure park|northgate chevrolet buick gmc ltd|northgate transmission centre|north hill auto service|nutri lawn|nyx professional makeup|oasis tirecraft|oil sands discovery centre|ok tire|optimum auto service|original joes restaurant bar|origins|paris jewellers|paris jewellers gift card|park2go|park2go airport rv parking self storage|park hyatt|parkland nurseries and garden centre|park n fly|parks canada|pedego electric bikes|penningtons|penske truck rental|pets plus us pet insurance|pioneer acres museum|pirelli tires inc|pizza 73|pods|poko popcorn|presidents choice|prfo|puzzle|rally|rapid brake|superstore|rec room|red arrow|reddy|reitmans|reliance|remedysrx|remington|carriage|renaissance|residence inn|reynolds alberta museum|richs garage|r j services ltd|roadrunners equipment|rocky mountain chocolate|rockys battery|rooster cafe kitchen|roots|royal canadian mint|running room gift card|rutherford|rw co|rwco|ryanco automotive|sacred arts wellness|sadie best western|samsung|sephora|sheraton hotels resorts|sherlocks automotive repair|sherwood flooring|shoppers drug mart beauty|shoppers drug mart gift card|shopping ca gift card|simons|simplicity car care calgary south|snow auto|soak luxury bath|sobeys safeway freshco iga gift card|southern autobody inc|southgate volkswagen|southwest auto service|sparkling hill resort|special event rentals|speedy apollo|speedy auto service|springhill suites|spruceland truck trailer|stallion van lines|staples|staples in store|state main kitchen bar|stephansson house provincial historic site|st john ambulance|stokes|st regis hotels resorts|sturgeon auto body shop|subaru|superior paint|suzy shier|swiss chalet|tapestry|teleflora|telus|science centre|telus world of science edmonton|the costume shoppe|the fairmont hotel macdonald|the globe and mail|the hangar flight museum|the hudsons bay company|the keg gift card|the last hunt|the right price auto|the ritz carlton|sherlock|the source|ultimate dining|think kitchen|thorncliffe|tiffanys|steak|tile town|tire kings|tolley tire|tom gerrys|total auto care|towneplace suites|trades automotive|trail tire auto centers|tribute|troubled monk|troy bilt|tutti frutti breakfast lunch|twilite music services|uber eats gift card|uber gift card|ukrainian cultural heritage village|vegreville mechanical|via rail|vib|vichy|vics service|victoria settlement provincial historic site|village auto tire services|vinces auto care|waldorf|walmart|tirecraft|warrens automotive|weddingstar|we kare|well ca|west end tire|westin|westside automotive|white s tire |w hotel|whyte|museum|wok|chevrolet|wolfe|hotel|collection|yardly|yoga|you4ia|^oj|combo|lasthint|via|horton|sephora|delta|^kal|nutri |gershaw|shell\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvAeQM8bQzOG"
      },
      "source": [
        "## **Upload File, Toggle De-Duping & Select Neg KW Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 492,
      "metadata": {
        "id": "9oA_ejVsPrgo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "3e19222e-ae0f-472e-faf5-9e0b1c9b7da5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-857f56d7-5b82-44a2-874b-ea5381b97655\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-857f56d7-5b82-44a2-874b-ea5381b97655\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving appended-competitor-data.xlsx to appended-competitor-data.xlsx\n"
          ]
        }
      ],
      "source": [
        "# upload the keyword export\n",
        "upload = files.upload()\n",
        "input_file = list(upload.keys())[0]  # get the name of the uploaded file\n",
        "# test the file extension\n",
        "file_extension = os.path.splitext(input_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 493,
      "metadata": {
        "id": "2ItHhRwErGkm"
      },
      "outputs": [],
      "source": [
        "# ---------------------------------- auto detect character encoding ----------------------------------------------------\n",
        "\n",
        "with open(input_file, 'rb') as rawdata:\n",
        "    result = chardet.detect(rawdata.read(10000))\n",
        "\n",
        "# if the encoding is utf-16 use a space separator, else ','\n",
        "if result['encoding'] == \"UTF-16\":\n",
        "    white_space = True\n",
        "else:\n",
        "    white_space = False\n",
        "\n",
        "if (\n",
        "    file_extension[1] == \".xlsx\"\n",
        "    or file_extension[1] == \".xls\"\n",
        "    or file_extension[1] == \".xlsm\"\n",
        "    or file_extension[1] == \".xlsb\"\n",
        "    or file_extension[1] == \".odf\"\n",
        "    or file_extension[1] == \".ods\"\n",
        "    or file_extension[1] == \".odt\"\n",
        "):\n",
        "    df_1 = pd.read_excel(input_file, engine=\"openpyxl\")\n",
        "else:\n",
        "    try:\n",
        "        df_1 = pd.read_csv(\n",
        "            input_file,\n",
        "            encoding=result[\"encoding\"],\n",
        "            delim_whitespace=white_space,\n",
        "            error_bad_lines=False,\n",
        "        )\n",
        "    # fall back to utf-8\n",
        "    except UnicodeDecodeError:\n",
        "        df_1 = pd.read_csv(\n",
        "            input_file,\n",
        "            encoding=\"utf-8\",\n",
        "            delim_whitespace=white_space,\n",
        "            error_bad_lines=False,\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {
        "id": "Nhk1iewZrH0f"
      },
      "outputs": [],
      "source": [
        "# -------------------------- check if single column import / and write header if missing -------------------------------\n",
        "\n",
        "# check the number of columns\n",
        "col_len = len(df_1.columns)\n",
        "col_name = df_1.columns[0]\n",
        "\n",
        "if col_len == 1 and df_1.columns[0] != \"Keyword\":\n",
        "    df_1.columns = [\"Keyword\"]\n",
        "\n",
        "if col_len == 1 and df_1.columns[0] != \"keyword\":\n",
        "    df_1.columns = [\"Keyword\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = input(\"Remove duplicate keywords? (y/n): \")\n",
        "if response.lower() == 'y':\n",
        "    df_1 = df_1.drop_duplicates(subset=['Keyword'], keep='first')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jk3xpc9TIjT",
        "outputId": "b6107534-75d7-49b4-d47e-a7e318b0cd5d"
      },
      "execution_count": 495,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Remove duplicate keywords? (y/n): y\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_negkw_set():\n",
        "    print(\"Select a set of negative keywords:\")\n",
        "    for i, key in enumerate(negkw_sets.keys()):\n",
        "        print(f\"{i + 1}. {key}\")\n",
        "\n",
        "    choice = input(\"Enter the number of your choice: \")\n",
        "    try:\n",
        "        selected_key = list(negkw_sets.keys())[int(choice) - 1]\n",
        "        return negkw_sets[selected_key]\n",
        "    except (IndexError, ValueError):\n",
        "        print(\"Invalid selection. Please enter a valid number.\")\n",
        "        return select_negkw_set()\n"
      ],
      "metadata": {
        "id": "bqPcXw03W-hk"
      },
      "execution_count": 496,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the selected set of negative keywords\n",
        "selected_negkw_filter = select_negkw_set()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOvptgAuYkow",
        "outputId": "8c346b9e-ef06-430d-e2eb-2ed16c4979a4"
      },
      "execution_count": 497,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Select a set of negative keywords:\n",
            "1. insurance\n",
            "2. registries\n",
            "3. DE\n",
            "Enter the number of your choice: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 498,
      "metadata": {
        "id": "UBlgRin3rJw9"
      },
      "outputs": [],
      "source": [
        "# -------------------------- detect if import file is adwords and remove the first two rows ----------------------------\n",
        "adwords_check = False\n",
        "if col_name == \"Search terms report\":\n",
        "    df_1.columns = df_1.iloc[1]\n",
        "    df_1 = df_1[1:]\n",
        "    df_1 = df_1.reset_index(drop=True)\n",
        "\n",
        "    new_header = df_1.iloc[0]  # grab the first row for the header\n",
        "    df_1 = df_1[1:]  # take the data less the header row\n",
        "    df_1.columns = new_header  # set the header row as the df header\n",
        "    adwords_check = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 499,
      "metadata": {
        "id": "cv0xmvs7rNwn"
      },
      "outputs": [],
      "source": [
        "# --------------------------------- Check if csv data is gsc and set bool ----------------------------------------------\n",
        "\n",
        "if 'Impressions' in df_1.columns:\n",
        "    gsc_data = True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 500,
      "metadata": {
        "id": "kPQa5Z-qrQFH"
      },
      "outputs": [],
      "source": [
        "# ----------------- standardise the column names between ahrefs v1/v2/semrush/gsc keyword exports ----------------------\n",
        "\n",
        "df_1.rename(\n",
        "    columns={\n",
        "        \"Current position\": \"Position\",\n",
        "        \"Current URL\": \"URL\",\n",
        "        \"Current URL inside\": \"Page URL inside\",\n",
        "        \"Current traffic\": \"Traffic\",\n",
        "        \"KD\": \"Difficulty\",\n",
        "        \"Keyword Difficulty\": \"Difficulty\",\n",
        "        \"Search Volume\": \"Volume\",\n",
        "        \"page\": \"URL\",\n",
        "        \"query\": \"Keyword\",\n",
        "        \"Top queries\": \"Keyword\",\n",
        "        \"Impressions\": \"Volume\",\n",
        "        \"Clicks\": \"Traffic\",\n",
        "        \"Search term\": \"Keyword\",\n",
        "        \"Impr.\": \"Volume\",\n",
        "        \"Search vol.\": \"Volume\",\n",
        "    },\n",
        "    inplace=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to detect language\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return \"error\"  # Returns 'error' if language detection fails\n",
        "\n",
        "# Apply the language detection function to the 'Keyword' column\n",
        "df_1['lang'] = df_1['Keyword'].apply(detect_language)\n",
        "\n",
        "# Filter out rows that are not detected as English\n",
        "#df_1 = df_1[df_1['lang'] == 'en']\n",
        "\n",
        "# Optionally, you can drop the 'lang' column if you no longer need it\n",
        "# df_1 = df_1.drop(columns=['lang'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-D0XkESZ5qS9"
      },
      "execution_count": 501,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 502,
      "metadata": {
        "id": "OGJtWEKOrQvm"
      },
      "outputs": [],
      "source": [
        "# ------------------------------ check number of imported rows and warn if excessive -----------------------------------\n",
        "\n",
        "row_len = len(df_1)\n",
        "if col_len > 1:\n",
        "    # --------------------------------- clean the data pre-grouping ----------------------------------------------------\n",
        "\n",
        "    if url_filter:\n",
        "        print(\"Processing only URLs containing:\", url_filter)\n",
        "\n",
        "    try:\n",
        "        df_1 = df_1[df_1[\"URL\"].str.contains(url_filter, na=False)]\n",
        "    except KeyError:\n",
        "        pass\n",
        "\n",
        "    # ========================= clean strings out of numerical columns (adwords) ========================================\n",
        "\n",
        "    try:\n",
        "        df_1[\"Volume\"] = df_1[\"Volume\"].str.replace(\",\", \"\").astype(int)\n",
        "        df_1[\"Traffic\"] = df_1[\"Traffic\"].str.replace(\",\", \"\").astype(int)\n",
        "        df_1[\"Conv. value / click\"] = df_1[\"Conv. value / click\"].str.replace(\",\", \"\").astype(float)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].str.replace(\",\", \"\").astype(float)\n",
        "        df_1[\"CTR\"] = df_1[\"CTR\"].replace(\" --\", \"0\", regex=True)\n",
        "        df_1[\"CTR\"] = df_1[\"CTR\"].str.replace(\"\\%\", \"\").astype(float)\n",
        "        df_1[\"Cost\"] = df_1[\"Cost\"].astype(float)\n",
        "        df_1[\"Conversions\"] = df_1[\"Conversions\"].astype(int)\n",
        "        df_1[\"Cost\"] = df_1[\"Cost\"].round(2)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].astype(float)\n",
        "        df_1[\"All conv. value\"] = df_1[\"All conv. value\"].round(2)\n",
        "\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    df_1 = df_1[~df_1[\"Keyword\"].str.contains(\"Total: \", na=False)]  # remove totals rows\n",
        "    df_1 = df_1[~df_1[\"Keyword\"].str.contains(\"td\", na=False)]  # remove TD rows\n",
        "    df_1 = df_1[~df_1[\"Keyword\"].str.contains(selected_negkw_filter, na=False)]  # remove neg kw rows\n",
        "    df_1 = df_1[~df_1[\"Keyword\"].str.contains(nonabkw_filter, na=False)]  # remove non-AB kw rows\n",
        "    df_1 = df_1[~df_1[\"Keyword\"].str.contains(commonlymissedfr_filter, na=False)]  # remove French kw rows the lang check seems to miss\n",
        "    df_1 = df_1[df_1[\"Keyword\"].notna()]  # keep only rows which are NaN\n",
        "    df_1 = df_1[df_1[\"Volume\"].notna()]  # keep only rows which are NaN\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].astype(str)\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].apply(lambda x: x.replace(\"0-10\", \"0\"))\n",
        "    df_1[\"Volume\"] = df_1[\"Volume\"].astype(float).astype(int)\n",
        "\n",
        "    # drop sitelinks\n",
        "\n",
        "    if drop_site_links:\n",
        "        try:\n",
        "            df_1 = df_1[~df_1[\"Page URL inside\"].str.contains(\"Sitelinks\", na=False)]  # drop sitelinks\n",
        "        except KeyError:\n",
        "            pass\n",
        "        try:\n",
        "            if gsc_data:\n",
        "                df_1 = df_1.sort_values(by=\"Traffic\", ascending=False)\n",
        "                df_1.drop_duplicates(subset=\"Keyword\", keep=\"first\", inplace=True)\n",
        "        except NameError:\n",
        "            pass\n",
        "\n",
        "    if drop_image_links:\n",
        "        try:\n",
        "            df_1 = df_1[~df_1[\"Page URL inside\"].str.contains(\"Image pack\", na=False)]  # drop image pack\n",
        "        except KeyError:\n",
        "            pass\n",
        "\n",
        "    df_1 = df_1[df_1[\"Volume\"] > min_volume]\n",
        "\n",
        "# start strip out all special characters from a column\n",
        "spec_chars = [\"!\",'\"',\"#\",\"%\",\"'\",\"(\",\")\",\n",
        "              \"*\",\"+\",\",\",\"-\",\".\",\"/\",\":\",\";\",\"<\",\n",
        "              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n",
        "              \"`\",\"{\",\"|\",\"}\",\"~\",\"–\"]\n",
        "for char in spec_chars:\n",
        "    df_1['Keyword'] = df_1['Keyword'].str.replace(char, ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "metadata": {
        "id": "LEptMghirbtA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc80582a-cd9a-4d00-94aa-3c646818c689"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning up the cluster tags.. Please be patient!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8910/8910 [00:08<00:00, 1046.35it/s]\n"
          ]
        }
      ],
      "source": [
        "# ------------------------------------- do the grouping ----------------------------------------------------------------\n",
        "\n",
        "df_1_list = df_1.Keyword.tolist()  # create list from df\n",
        "model = PolyFuzz(\"TF-IDF\")\n",
        "\n",
        "cluster_tags = df_1_list[::]\n",
        "cluster_tags = set(cluster_tags)\n",
        "cluster_tags = list(cluster_tags)\n",
        "\n",
        "print(\"Cleaning up the cluster tags.. Please be patient!\")\n",
        "substrings = {w1 for w1 in tqdm(cluster_tags) for w2 in cluster_tags if w1 in w2 and w1 != w2}\n",
        "longest_word = set(cluster_tags) - substrings\n",
        "longest_word = list(longest_word)\n",
        "shortest_word_list = list(set(cluster_tags) - set(longest_word))\n",
        "\n",
        "try:\n",
        "    model.match(df_1_list, shortest_word_list)\n",
        "except ValueError:\n",
        "    print(\"Empty Dataframe, Can't Match - Check the URL Filter!\")\n",
        "    sys.exit()\n",
        "\n",
        "model.group(link_min_similarity=sim_match_percent)\n",
        "df_matched = model.get_matches()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "metadata": {
        "id": "OlD59vIsreG2"
      },
      "outputs": [],
      "source": [
        "# ------------------------------- clean the data post-grouping ---------------------------------------------------------\n",
        "\n",
        "df_matched.rename(columns={\"From\": \"Keyword\", \"Group\": \"Cluster Name\"}, inplace=True)  # renaming multiple columns\n",
        "\n",
        "# merge keyword volume / CPC / Pos / URL etc data from original dataframe back in\n",
        "df_matched = pd.merge(df_matched, df_1, on=\"Keyword\", how=\"left\")\n",
        "\n",
        "# rename traffic (acs) / (desc) to 'Traffic for standardisation\n",
        "df_matched.rename(columns={\"Traffic (desc)\": \"Traffic\", \"Traffic (asc)\": \"Traffic\", \"Traffic potential\": \"Traffic\"}, inplace=True)\n",
        "\n",
        "if col_len > 1:\n",
        "\n",
        "    # fill in missing values\n",
        "    df_matched.fillna({\"Traffic\": 0, \"CPC\": 0}, inplace=True)\n",
        "    df_matched['Traffic'] = df_matched['Traffic'].round(0)\n",
        "    # ------------------------- group the data and merge in original stats -------------------------------------------------\n",
        "    if not adwords_check:\n",
        "        try:\n",
        "            # make dedicated grouped dataframe\n",
        "            df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "                {\"Volume\": sum, \"Difficulty\": \"median\", \"CPC\": \"median\", \"Traffic\": sum}).reset_index())\n",
        "        except Exception:\n",
        "            df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "                {\"Volume\": sum, \"Traffic\": sum}).reset_index())\n",
        "\n",
        "        df_grouped = df_grouped.rename(\n",
        "            columns={\"Volume\": \"Cluster Volume\", \"Difficulty\": \"Cluster KD (Median)\", \"CPC\": \"Cluster CPC (Median)\",\n",
        "                     \"Traffic\": \"Cluster Traffic\"})\n",
        "\n",
        "        df_matched = pd.merge(df_matched, df_grouped, on=\"Cluster Name\", how=\"left\")  # merge in the group stats\n",
        "\n",
        "    if adwords_check:\n",
        "\n",
        "        df_grouped = (df_matched.groupby(\"Cluster Name\").agg(\n",
        "            {\"Volume\": sum, \"CTR\": \"median\", \"Cost\": sum, \"Traffic\": sum, \"All conv. value\": sum, \"Conversions\": sum}).reset_index())\n",
        "\n",
        "        df_grouped = df_grouped.rename(\n",
        "            columns={\"Volume\": \"Cluster Volume\", \"CTR\": \"Cluster CTR (Median)\", \"Cost\": \"Cluster Cost (Sum)\",\n",
        "                     \"Traffic\": \"Cluster Traffic\", \"All conv. value\": \"All conv. value (Sum)\", \"Conversions\": \"Cluster Conversions (Sum)\"})\n",
        "\n",
        "        df_matched = pd.merge(df_matched, df_grouped, on=\"Cluster Name\", how=\"left\")  # merge in the group stats\n",
        "\n",
        "        del df_matched['To']\n",
        "        del df_matched['Similarity']\n",
        "\n",
        "    # ---------------------------- clean and sort the final output -----------------------------------------------------\n",
        "\n",
        "    try:\n",
        "        df_matched.drop_duplicates(subset=[\"URL\", \"Keyword\"], keep=\"first\", inplace=True)  # drop if both kw & url are duped\n",
        "\n",
        "    except KeyError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "metadata": {
        "id": "O4Svmxokri1O"
      },
      "outputs": [],
      "source": [
        "if not adwords_check:\n",
        "    cols = (\n",
        "        \"Cluster Name\",\n",
        "        \"Keyword\",\n",
        "        \"Cluster Size\",\n",
        "        \"Cluster Volume\",\n",
        "        \"Cluster KD (Median)\",\n",
        "        \"Cluster CPC (Median)\",\n",
        "        \"Cluster Traffic\",\n",
        "        \"Volume\",\n",
        "        \"Difficulty\",\n",
        "        \"CPC\",\n",
        "        \"Traffic\",\n",
        "        \"URL\",\n",
        "    )\n",
        "\n",
        "    df_matched = df_matched.reindex(columns=cols)\n",
        "\n",
        "    try:\n",
        "        if gsc_data:\n",
        "            cols = \"Cluster Name\", \"Keyword\", \"Cluster Size\", \"Cluster Volume\", \"Cluster Traffic\", \"Volume\", \"Traffic\"\n",
        "            df_matched = df_matched.reindex(columns=cols)\n",
        "    except NameError:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "metadata": {
        "id": "Ib69NIuwrj2-"
      },
      "outputs": [],
      "source": [
        "# ------------ get the keyword with the highest search volume to replace the auto generated tag name with --------------\n",
        "\n",
        "if col_len > 1:\n",
        "    if parent_by_vol:\n",
        "        df_matched['vol_max'] = df_matched.groupby(['Cluster Name'])['Volume'].transform(max)\n",
        "        # this sort is mandatory for the renaming to work properly by floating highest values to the top of the cluster\n",
        "        df_matched.sort_values([\"Cluster Name\", \"Cluster Volume\", \"Volume\"], ascending=[False, True, False], inplace=True)\n",
        "        df_matched['exact_vol_match'] = df_matched['vol_max'] == df_matched['Volume']\n",
        "        df_matched.loc[df_matched['exact_vol_match'] == True, 'highest_ranked_keyword'] = df_matched['Keyword']\n",
        "        df_matched['highest_ranked_keyword'] = df_matched['highest_ranked_keyword'].fillna(method='ffill')\n",
        "        df_matched['Cluster Name'] = df_matched['highest_ranked_keyword']\n",
        "        del df_matched['vol_max']\n",
        "        del df_matched['exact_vol_match']\n",
        "        del df_matched['highest_ranked_keyword']\n",
        "if adwords_check:\n",
        "    df_matched = df_matched.rename(columns={\"Volume\": \"Impressions\", \"Traffic\": \"Clicks\", \"Cluster Traffic\": \"Cluster Clicks (Sum)\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "metadata": {
        "id": "xe76sl5Vror2"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------- final output ------------------------------------------------------------------\n",
        "# sort on cluster size\n",
        "df_matched.sort_values([\"Cluster Size\", \"Cluster Name\", \"Cluster Volume\"], ascending=[False, True, False], inplace=True)\n",
        "\n",
        "try:\n",
        "    if gsc_data:\n",
        "        df_matched.rename(\n",
        "            columns={\"Cluster Volume\": \"Cluster Impressions\", \"Cluster Traffic\": \"Cluster Clicks\", \"Traffic\": \"Clicks\",\n",
        "                     \"Volume\": \"Impressions\"}, inplace=True)\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "if col_len == 1:\n",
        "    cols = \"Cluster Name\", \"Keyword\", \"Cluster Size\"\n",
        "    df_matched = df_matched.reindex(columns=cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "metadata": {
        "id": "4QFzhJmHXIPa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77e2f82a-8c42-455a-de0a-71f3065e027b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Cluster Name                                            Keyword  \\\n",
            "527     ca domain                                          ca domain   \n",
            "924     ca domain                                          ca domain   \n",
            "2527    ca domain                                          at domain   \n",
            "2798    ca domain                                             donain   \n",
            "2825    ca domain                              domain purchase sites   \n",
            "...           ...                                                ...   \n",
            "6053   your hobby  how to pay yourself dividends from your corpor...   \n",
            "6321   your hobby          how to pay yourself from your corporation   \n",
            "7739   your hobby                        how to incorporate yourself   \n",
            "8785   your hobby                 paying yourself from your business   \n",
            "8799   your hobby                beyond yourself peanut butter dream   \n",
            "\n",
            "      Cluster Size  Cluster Volume  Cluster KD (Median)  Cluster CPC (Median)  \\\n",
            "527            NaN          1620.0                 38.5                  4.02   \n",
            "924            NaN          1620.0                 38.5                  4.02   \n",
            "2527           NaN          1620.0                 38.5                  4.02   \n",
            "2798           NaN          1620.0                 38.5                  4.02   \n",
            "2825           NaN          1620.0                 38.5                  4.02   \n",
            "...            ...             ...                  ...                   ...   \n",
            "6053           NaN           510.0                 30.0                  2.93   \n",
            "6321           NaN           510.0                 30.0                  2.93   \n",
            "7739           NaN           510.0                 30.0                  2.93   \n",
            "8785           NaN           510.0                 30.0                  2.93   \n",
            "8799           NaN           510.0                 30.0                  2.93   \n",
            "\n",
            "      Cluster Traffic  Volume  Difficulty   CPC  Traffic  \\\n",
            "527               3.0     590        40.0  6.70      0.0   \n",
            "924               3.0     320        39.0  6.70      0.0   \n",
            "2527              3.0     140        29.0  1.47      0.0   \n",
            "2798              3.0     110        96.0  3.38      0.0   \n",
            "2825              3.0     110        81.0  8.26      0.0   \n",
            "...               ...     ...         ...   ...      ...   \n",
            "6053              0.0      50        24.0  0.00      0.0   \n",
            "6321              0.0      50        29.0  8.87      0.0   \n",
            "7739              0.0      30        31.0  2.38      0.0   \n",
            "8785              0.0      30        33.0  3.48      0.0   \n",
            "8799              0.0      30         5.0  0.33      0.0   \n",
            "\n",
            "                                                    URL  \n",
            "527   https://businesslink.ca/ca-or-com-which-should...  \n",
            "924   https://www.bdc-canada.com/BDC/services/Web-Ho...  \n",
            "2527  https://businesslink.ca/what-are-domain-names-...  \n",
            "2798  https://www.bdc-canada.com/BDC/services/Web-Ho...  \n",
            "2825  https://www.bdc-canada.com/BDC/services/Web-Ho...  \n",
            "...                                                 ...  \n",
            "6053  https://businesslink.ca/how-to-pay-yourself-as...  \n",
            "6321  https://businesslink.ca/how-to-pay-yourself-as...  \n",
            "7739  https://www.bdc-canada.com/BDC/services/Incorp...  \n",
            "8785  https://businesslink.ca/how-to-pay-yourself-as...  \n",
            "8799  https://businesslink.ca/blog/financing-your-bu...  \n",
            "\n",
            "[8917 rows x 12 columns]\n"
          ]
        }
      ],
      "source": [
        "print(df_matched)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "metadata": {
        "id": "_EvJIgxWOdaI"
      },
      "outputs": [],
      "source": [
        "# - add in intent markers\n",
        "colname = df_matched.columns[1]\n",
        "df_matched.loc[df_matched[colname].str.contains(info_filter), \"Informational\"] = \"Informational\"\n",
        "df_matched.loc[df_matched[colname].str.contains(comm_invest_filter), \"Commercial Investigation\"] = \"Commercial Investigation\"\n",
        "df_matched.loc[df_matched[colname].str.contains(trans_filter), \"Transactional\"] = \"Transactional\"\n",
        "df_matched.loc[df_matched[colname].str.contains(alberta_filter), \"Alberta\"] = \"Alberta\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(cities_filter), \"Cities\"] = \"Cities\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(nearme_filter), \"Near Me\"] = \"Near Me\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(neighbor_filter), \"Neighboring City\"] = \"Neighboring City\"\n",
        "df_matched.loc[df_matched[colname].str.contains(branded_filter), \"Branded\"] = \"Branded\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(insurance_filter), \"Insurance\"] = \"Insurance\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "metadata": {
        "id": "36k35A8VGX6A"
      },
      "outputs": [],
      "source": [
        "# - add in cluster markers\n",
        "#colname = df_matched.columns[1]\n",
        "#df_matched.loc[df_matched[colname].str.contains(vehicle_filter), \"Vehicle\"] = \"Vehicle\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(property_filter), \"Property\"] = \"Property\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(pet_filter), \"Pet\"] = \"Pet\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(health_filter), \"Life & Health\"] = \"Life & Health\"\n",
        "#df_matched.loc[df_matched[colname].str.contains(commercial_filter), \"Business\"] = \"Business\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_matched['detected_language'] = df_matched['Keyword'].apply(detect_language)"
      ],
      "metadata": {
        "id": "ljtwYHMULQ1K"
      },
      "execution_count": 511,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate the spell checker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Add custom words that are known to be correct\n",
        "spell.word_frequency.load_words(['alberta', 'nuans', 'cra', 'nebs', 'provincially', 'vs', 'inc', 'llc', 'npo', 'tm', 'proprietorship', 'calgary', 'kpi', 'gst', 'edmonton', 'proprietorships', 'wcb', 'cpa'])\n",
        "\n",
        "# Define a function to find misspellings, ignoring case\n",
        "def find_misspellings(text):\n",
        "    # Convert text to lower case\n",
        "    words = text.lower().split()\n",
        "    misspelled = spell.unknown(words)\n",
        "    return \", \".join(misspelled)\n",
        "\n",
        "# Apply the function to the 'Keyword' column (ensure this column name matches your DataFrame)\n",
        "df_matched['Review'] = df_matched['Keyword'].apply(find_misspellings)\n",
        "\n",
        "\n",
        "# Now, df_matched will have an additional column 'Review'\n"
      ],
      "metadata": {
        "id": "CJXstFrKA3xl"
      },
      "execution_count": 512,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "metadata": {
        "id": "bDpePB7_8xyk"
      },
      "outputs": [],
      "source": [
        "# find keywords from one column in another in any order and count the frequency\n",
        "df_matched['Cluster Name'] = df_matched['Cluster Name'].str.strip()\n",
        "df_matched['Keyword'] = df_matched['Keyword'].str.strip()\n",
        "\n",
        "df_matched['First Word'] = df_matched['Cluster Name'].str.split(\" \").str[0]\n",
        "df_matched['Second Word'] = df_matched['Cluster Name'].str.split(\" \").str[1]\n",
        "df_matched['Total Keywords'] = df_matched['First Word'].str.count(' ') + 1\n",
        "\n",
        "def ismatch(s):\n",
        "    A = set(s[\"First Word\"].split())\n",
        "    B = set(s['Keyword'].split())\n",
        "    return A.intersection(B) == A\n",
        "\n",
        "df_matched['Found'] = df_matched.apply(ismatch, axis=1)\n",
        "\n",
        "df_matched = df_matched. fillna('')\n",
        "\n",
        "def ismatch(s):\n",
        "    A = set(s[\"Second Word\"].split())\n",
        "    B = set(s['Keyword'].split())\n",
        "    return A.intersection(B) == A\n",
        "df_matched['Found 2'] = df_matched.apply(ismatch, axis=1)\n",
        "\n",
        "# todo - document this algo. Essentially if it matches on the second word only, it renames the cluster to the second word\n",
        "# clean up code nd variable names\n",
        "\n",
        "df_matched.loc[(df_matched[\"Found\"] == False) & (df_matched[\"Found 2\"] == True), \"Cluster Name\"] = df_matched[\"Second Word\"]\n",
        "df_matched.loc[(df_matched[\"Found\"] == False) & (df_matched[\"Found 2\"] == False), \"Cluster Name\"] = \"zzz_no_cluster_available\"\n",
        "\n",
        "# count cluster_size\n",
        "df_matched['Cluster Size'] = df_matched['Cluster Name'].map(df_matched.groupby('Cluster Name')['Cluster Name'].count())\n",
        "df_matched.loc[df_matched[\"Cluster Size\"] == 1, \"Cluster Name\"] = \"zzz_no_cluster_available\"\n",
        "\n",
        "\n",
        "df_matched = df_matched.sort_values(by=\"Cluster Name\", ascending=True)\n",
        "\n",
        "#delete the helper cols\n",
        "del df_matched['First Word']\n",
        "del df_matched['Second Word']\n",
        "del df_matched['Total Keywords']\n",
        "del df_matched['Found']\n",
        "del df_matched['Found 2']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "metadata": {
        "id": "zoLLPJlxOf_3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "a3f28b2c-6094-4dfd-ecbd-3e5ef5c05b2f"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c47093fb-cff1-46c8-90cc-7fb43c59346e\", \"cleaned_clustered_ceywords.csv\", 1483575)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df_matched.to_csv('cleaned_clustered_ceywords.csv', index=False)\n",
        "files.download(\"cleaned_clustered_ceywords.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}